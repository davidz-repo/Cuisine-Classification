{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Qixiang Zhang  \n",
    "Jul 3rd, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [*What's Cooking?*](https://www.kaggle.com/c/whats-cooking) |  [Kaggle](https://www.kaggle.com/)\n",
    "A featured [Kaggle](https://www.kaggle.com/) playground competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cover](pic/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is built upon one of [Kaggle](https://www.kaggle.com/)'s playground competition - [*What's Cooking?*](https://www.kaggle.com/c/whats-cooking).\n",
    "\n",
    "Grew up in China, studied in the United States for 10 years, traveled to some parts of the world, and inspired by Anthony Bourdain, I love eating and making food. Being a home-cook myself, I always enjoy mixing a few ingredients to produce meals that my family love (most of the time).\n",
    "\n",
    "Thank [Yummly](https://www.yummly.com/) for the dataset and [Kaggle](https://www.kaggle.com/) for hosting the competition. I will leverage what I have learned from [Udacity](https://www.udacity.com/)'s Machine Learning Nanodegree program to explore machine learning models and produce the optimal results for this capstone project.\n",
    "\n",
    "Even though this is a passed competition, it is listed as a playground. I have registered with this competition and here is my Kaggle [page](https://www.kaggle.com/dzkaggle).\n",
    "\n",
    "I have also found these cool papers and sources below regarding multiclass classification:\n",
    "- [Survey on Multiclass Classification Methods](https://www.cs.utah.edu/~piyush/teaching/aly05multiclass.pdf)\n",
    "- [A Comparison of Methods for Multi-class Support Vector Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)\n",
    "- [Towards Maximizing the Area Under the ROC Curve for Multi-Class Classification Problems](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3485/3882)\n",
    "- [A simplified extension of the Area under the ROC to the multiclass domain](https://pdfs.semanticscholar.org/dc70/1e7fca147e2bf37f14481e35e1b975396809.pdf)\n",
    "- [L1 AND L2 REGULARIZATION FOR MULTICLASS HINGE LOSS MODELS](https://pdfs.semanticscholar.org/e82c/817c75061797f2ec111610ae4ce8d8c20794.pdf)\n",
    "- [On Logistic Regression: Gradients of the Log Loss, Multi-Class Classification, and Other Optimization Techniques](http://ttic.uchicago.edu/~suriya/website-intromlss2018/course_material/Day3b.pdf)\n",
    "- [Machine Learning Basics Lecture 7: Multiclass Classification](https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture7_multiclass.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal is to categorize the cuisine by using recipe ingredients. Specifically, to answer these questions below:\n",
    " - What are the most used ingredients in each of the cuisines?\n",
    " - How similar or different are these cuisines?\n",
    " - Given a recipe (ingredient list), how accurately can the model classify its cuisine?\n",
    "\n",
    "From the labeled dataset, we can learn: what are the most used ingredients, main ingredients for each cuisine, which cuisines are similar to each other (consists of similar ingredients), and finally use recipe ingredients to predict the most probable cuisines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Originally, I was going to use AUC_ROC to measure the performance of my models because the class samples are imbalanced. For example, Italian cuisine has 7838 recipes (19.71%) while Brazillian cuisine has only 467 recipes (1.17%). However, I decided to stick with the Kaggle's metric \"accuracy\" to measure the performances of my model for two reasons: the results will be measured under Kaggle's evaludation methodology and AUC_ROC may give my model a different result; AUC_ROC is very complicated to implement across all the models I will be using. To compensate the imbalanced data, I will split the train-validation data manually to maintain the weights for each class. I will also use sklearn's StratifiedKFold to cross validate the models while doing the grid search for the best parameters for each model.\n",
    "\n",
    "The main goal is to be able to use an ingredients list to predict the most probable cuisine (e.g. Greek, Indian, Chinese, etc.) out of 20 total possible cuisines. The training dataset is clean and labeled. I can directly use defined models to train on the dataset and to classify recipes into 20 different classes.\n",
    "\n",
    "The multiclass classification problem can be decomposed into several binary classification tasks that can be solved efficiently using binary classifiers [Multiclass Classification], I will be using some of the scikit-learn modules as follows:\n",
    "- [Gaussian Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
    "- Ensemble Methods ([Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), [XGBoost](https://github.com/dmlc/xgboost), and [LightGBM](https://github.com/Microsoft/LightGBM))\n",
    "- [Stochastic Gradient Descent Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n",
    "- [Support Vector Machines](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "- [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n",
    "\n",
    "Last, I will design a deep neural network from [Keras](https://keras.io/) (inspired by [Jason Brownlee's article](https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/))\n",
    "\n",
    "[Multiclass Classification]: https://www.cs.utah.edu/~piyush/teaching/aly05multiclass.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Steps\n",
    "The workflow for solving the problem would follow the similar approach that provided by other projects in the Machine Learning Nanodegree:  \n",
    " 1. Import the data\n",
    " 2. Study the data\n",
    " 3. Preprocess the data\n",
    " 4. Explore different models\n",
    " 5. Evaluate the models\n",
    " 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### EXPLORE #########==================\n",
    "# data exploring and basic libraries\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from collections import deque as dq\n",
    "\n",
    "# NLP preprocessing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize as TK\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##### MODELING ######===================\n",
    "# from time import time\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "# model eval\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# basic multi-class classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# binary class classification models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# One vs All wrapper\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# additional multi-class classification models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "###### supplementary code#######\n",
    "\n",
    "# Pretty display for notebooks\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>[sausage links, fennel bulb, fronds, olive oil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...\n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...\n",
       "2  41580  [sausage links, fennel bulb, fronds, olive oil..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data - test data\n",
    "rawdf_te = pd.read_json(path_or_buf='raw_data/test.json')\n",
    "rawdf_te.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data - train data\n",
    "rawdf_tr = pd.read_json(path_or_buf='raw_data/train.json')\n",
    "rawdf_tr.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\t Recipe\t Cuisine\n",
      "\n",
      "19.71 %\t 7838 \t italian\n",
      "16.19 %\t 6438 \t mexican\n",
      "10.86 %\t 4320 \t southern_us\n",
      "7.55 %\t 3003 \t indian\n",
      "6.72 %\t 2673 \t chinese\n",
      "6.65 %\t 2646 \t french\n",
      "3.89 %\t 1546 \t cajun_creole\n",
      "3.87 %\t 1539 \t thai\n",
      "3.58 %\t 1423 \t japanese\n",
      "2.95 %\t 1175 \t greek\n",
      "2.49 %\t 989 \t spanish\n",
      "2.09 %\t 830 \t korean\n",
      "2.07 %\t 825 \t vietnamese\n",
      "2.06 %\t 821 \t moroccan\n",
      "2.02 %\t 804 \t british\n",
      "1.9 %\t 755 \t filipino\n",
      "1.68 %\t 667 \t irish\n",
      "1.32 %\t 526 \t jamaican\n",
      "1.23 %\t 489 \t russian\n",
      "1.17 %\t 467 \t brazilian\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cuisine distribution\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(25, 12), tight_layout=0)\n",
    "ax = sns.countplot(y='cuisine', data=rawdf_tr, palette ='Set2')\n",
    "ax.set_title(label='Recipe Distribution', fontdict={'fontsize':35}, pad=10)\n",
    "\n",
    "# number of recipes for each cuisines\n",
    "print('Weight\\t Recipe\\t Cuisine\\n')\n",
    "for _ in (Counter(rawdf_tr['cuisine']).most_common()):print(round(_[1]/rawdf_tr.cuisine.count()*100, 2),'%\\t',_[1],'\\t', _[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 39774 recipes\n",
      "\n",
      "Total of 20 types of cuisines including ['greek', 'southern_us', 'filipino', 'indian', 'jamaican', 'spanish', 'italian', 'mexican', 'chinese', 'british', 'thai', 'vietnamese', 'cajun_creole', 'brazilian', 'french', 'japanese', 'irish', 'korean', 'moroccan', 'russian']\n",
      "\n",
      "Total of 6714 unique ingredients\n",
      "\n",
      "Most common ingredients used:\n",
      "\n",
      "('salt', 18049)\n",
      "('onions', 7972)\n",
      "('olive oil', 7972)\n",
      "('water', 7457)\n",
      "('garlic', 7380)\n",
      "('sugar', 6434)\n",
      "('garlic cloves', 6237)\n",
      "('butter', 4848)\n",
      "('ground black pepper', 4785)\n",
      "('all-purpose flour', 4632)\n",
      "('pepper', 4438)\n"
     ]
    }
   ],
   "source": [
    "# change id column type to string\n",
    "rawdf_tr = rawdf_tr.set_index('id')\n",
    "rawdf_te = rawdf_te.set_index('id')\n",
    "\n",
    "# training ingredient list\n",
    "ingredients_list_tr = []\n",
    "for _ in rawdf_tr['ingredients']:\n",
    "    ingredients_list_tr.append(_)\n",
    "# ingredients set - ingredients_set()\n",
    "ingredients_set_tr = set()\n",
    "for a in range(len(ingredients_list_tr)):\n",
    "    for _ in range(len(ingredients_list_tr[a])):\n",
    "        ingredients_set_tr.add(ingredients_list_tr[a][_])\n",
    "\n",
    "# Total number of recipes\n",
    "print('Total of %d recipes\\n'% len(rawdf_tr))\n",
    "\n",
    "# total number of UNIQUE cuisines\n",
    "print('Total of %d types of cuisines including %s\\n' % \\\n",
    "      (len(rawdf_tr['cuisine'].unique()), rawdf_tr['cuisine'].unique().tolist()))\n",
    "                                          \n",
    "# unique ingredients (raw training data)\n",
    "print(\"Total of %d unique ingredients\\n\" % len(ingredients_set_tr))\n",
    "\n",
    "# total ingredients list (with repition) occurred in the train data\n",
    "total_ingredients_list_tr = []\n",
    "for i in range(len(ingredients_list_tr)):\n",
    "    for j in range(len(ingredients_list_tr[i])):\n",
    "        total_ingredients_list_tr.append(ingredients_list_tr[i][j])\n",
    "print(\"Most common ingredients used:\\n\")\n",
    "for _ in range(len(Counter(total_ingredients_list_tr).most_common(11))):\n",
    "    print(Counter(total_ingredients_list_tr).most_common(11)[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### How similar are these cuisines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this [graph in the competition kernel page](https://www.kaggle.com/alonalevy/cultural-diffusion-by-recipes/notebook) posted by [alona_levy](https://www.kaggle.com/alonalevy) completed this question beautifully. All kudos to him showing the relationships between cuisines. The image is included as below:\n",
    "![cuisines_bubble](pic/cuisines_bubble.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Algorithms and Techniques\n",
    "Within the ingredient list, although I have consoladated each ingredient from every recipe, some of the differentiated ingredients might still be the same (i.e. garlic vs. garlic cloves). Natural language processing is recommended for training better models. The option will be explored and tested after implementing the benchmark model - random forest.\n",
    "\n",
    "Additionally, we can see that the classes are imbalanced. Some of the cuisines have more recipes (or training data) than others. Therefore, [Accuracy may not be the appropriate metric](https://towardsdatascience.com/accuracy-paradox-897a69e2dd9b) for measuring the performances of each model, however, Kaggle uses accuracy to score every competitor. I will mannually split the training dataset into training and hold-out testing to best maintain the weights for each class. Then, I will use [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to cross validate each model and [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to tune each model. Lastly, I will use accuracy as a scoring function within the grid search method to measure the performance of each model under different set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark - Random Forest (sklearn.ensemble.[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
    "\n",
    "Random forest is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. The nature of random forest is perfect for multi-class classification.\n",
    "\n",
    "For the benchmark model, I will be implementing the random forest model out of the box without tuning the parameters. Let's see how well the model does in the later section after preprocessing the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Preprocess the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the ingredient column stores each indgredients in a list. I also found there are some characters that are not useful for classification (i.e. '()', numbers, '¢', etc.). One way to get rid of them is to use regular expression functions to find the pattern and remove the non-letter characters. Another powerful library that I found through the [source1](https://machinelearningmastery.com/clean-text-machine-learning-python/) and [source2](https://www.kaggle.com/futurist/text-preprocessing-and-machine-learning-modeling) is to use natural language process package [ntlk](https://www.nltk.org/api/nltk.stem.html).\n",
    "\n",
    "Let's look at in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the ingredients column look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(    oz.) tomato sauce', 'serrano chilies', 'golden raisins', 'firmly packed brown sugar', 'chopped garlic', 'vinegar']\n",
      "['manchego cheese', 'empanada', 'jalapeno chilies', 'garlic salt', '2 1/2 to 3 lb. chicken, cut into serving pieces', \"hellmann' or best food real mayonnais\", 'vegetable oil', 'sliced green onions']\n",
      "['butter', 'sugar', 'corn starch', 'Betty Crockerâ„¢ oatmeal cookie mix', 'YoplaitÂ® Greek 100 blackberry pie yogurt', 'blackberries']\n",
      "['Tipo 00 flour', 'semolina', 'extra-virgin olive oil', 'large free range egg']\n"
     ]
    }
   ],
   "source": [
    "print(rawdf_tr['ingredients'].loc[41935])\n",
    "print(rawdf_tr['ingredients'].loc[27566])\n",
    "print(rawdf_tr['ingredients'].loc[32596])\n",
    "print(rawdf_tr['ingredients'].loc[8476])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ingredients above are small samples in the training dataset. We have to preprocess the data to clean the 'ingredients' going into the models before training.\n",
    "\n",
    "I will use [regular expression](https://docs.python.org/2/library/re.html) to get rid of non-letter characters, and then [nltk](https://www.nltk.org/api/nltk.stem.html) to restore words to their original forms (i.e. 'tomatoes -> tomato') \n",
    "\n",
    "1. I found this [tutorial](https://www.youtube.com/watch?v=K8L6KVGG-7o) very helpful for learning the basics about the regular expression library.\n",
    "2. I learned most about **lemmatizing** and **stemming** from [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the series from the dataframe\n",
    "ingredients_tr = rawdf_tr['ingredients']\n",
    "# do the test.json while at it\n",
    "ingredients_te = rawdf_te['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the matched pattern\n",
    "def sub_match(pattern, sub_pattern, ingredients):\n",
    "    for i in ingredients.index.values:\n",
    "        for j in range(len(ingredients[i])):\n",
    "            ingredients[i][j] = re.sub(pattern, sub_pattern, ingredients[i][j].strip())\n",
    "            ingredients[i][j] = ingredients[i][j].strip()\n",
    "    re.purge()\n",
    "    return ingredients\n",
    "\n",
    "def regex_sub_match(series):\n",
    "    # remove all units\n",
    "    p0 = re.compile(r'\\s*(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\s*[^a-z]')\n",
    "    series = sub_match(p0, ' ', series)\n",
    "    # remove all digits\n",
    "    p1 = re.compile(r'\\d+')\n",
    "    series = sub_match(p1, ' ', series)\n",
    "    # remove all the non-letter characters\n",
    "    p2 = re.compile('[^\\w]')\n",
    "    series = sub_match(p2, ' ', series)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex both train and test data\n",
    "ingredients_tr = regex_sub_match(ingredients_tr)\n",
    "ingredients_te = regex_sub_match(ingredients_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do those previous examples look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toma sauce', 'serrano chilies', 'golden raisins', 'firmly packed brown sugar', 'chopped garlic', 'vinegar']\n",
      "['manchego cheese', 'empanada', 'jalapeno chilies', 'garlic salt', 'chicken  cut in serving pieces', 'hellmann  or best food real mayonnais', 'vegetable oil', 'sliced green onions']\n",
      "['butter', 'sugar', 'corn starch', 'Betty Crockerâ   oatmeal cookie mix', 'YoplaitÂ  Greek   blackberry pie yogurt', 'blackberries']\n",
      "['Tipo   flour', 'semolina', 'extra virgin olive oil', 'large free range egg']\n"
     ]
    }
   ],
   "source": [
    "print(ingredients_tr[41935])\n",
    "print(ingredients_tr[27566])\n",
    "print(ingredients_tr[32596])\n",
    "print(ingredients_tr[8476])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! Now let's use the lemmatizer to restore the words to their original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare instance from WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove all the words that are not nouns -- keep the essential ingredients\n",
    "def lemma(series):\n",
    "    for i in series.index.values:\n",
    "        for j in range(len(series[i])):\n",
    "            # get rid of all extra spaces\n",
    "            series[i][j] = series[i][j].strip()\n",
    "            # Tokenize a string to split off punctuation other than periods\n",
    "            token = TK(series[i][j])\n",
    "            # set all the plural nouns into singular nouns\n",
    "            for k in range(len(token)):\n",
    "                token[k] = lemmatizer.lemmatize(token[k])\n",
    "            token = ' '.join(token)\n",
    "            # write them back\n",
    "            series[i][j] = token\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize both train and test data\n",
    "ingredients_tr = lemma(ingredients_tr)\n",
    "ingredients_te = lemma(ingredients_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the preprocessed results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toma sauce', 'serrano chilies', 'golden raisin', 'firmly packed brown sugar', 'chopped garlic', 'vinegar']\n",
      "['manchego cheese', 'empanada', 'jalapeno chilies', 'garlic salt', 'chicken cut in serving piece', 'hellmann or best food real mayonnais', 'vegetable oil', 'sliced green onion']\n",
      "['butter', 'sugar', 'corn starch', 'Betty Crockerâ oatmeal cookie mix', 'YoplaitÂ Greek blackberry pie yogurt', 'blackberry']\n",
      "['Tipo flour', 'semolina', 'extra virgin olive oil', 'large free range egg']\n"
     ]
    }
   ],
   "source": [
    "print(ingredients_tr[41935])\n",
    "print(ingredients_tr[27566])\n",
    "print(ingredients_tr[32596])\n",
    "print(ingredients_tr[8476])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy back to the dataframe\n",
    "rawdf_tr['ingredients_lemma'] = ingredients_tr\n",
    "rawdf_tr['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_tr['ingredients_lemma']]\n",
    "# do the same for the test.json dataset\n",
    "rawdf_te['ingredients_lemma'] = ingredients_te\n",
    "rawdf_te['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_te['ingredients_lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, I was going to remove all the words except for nouns. However, I found the nltk library does not usually pos_tag each word correctly (i.e. the word 'chicken' was tagged as a verb instead of a noun). Also, removing these words might throw away some important data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### Implementation - further preprocessing  \n",
    "  \n",
    "I found another very useful technique: [Tf-IDF or term frequency–inverse document frequency](https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). According to [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), it is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Tf-idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf-idf.\n",
    "\n",
    "Now the input data for both training and testing have been preprocessed by removing units and some of the non-letter characters such as numbers and whitespaces. It still needs to be configured into a tf-idf matrix before feeding into different models. Instead of using strings as input, I will be using the [tfidfvectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from sklearn to configure the strings into numbers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39774, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients_lemma_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>romaine lettuce black olive grape tomato garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>plain flour ground pepper salt tomato ground b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>egg pepper salt mayonaise cooking oil green ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>water vegetable oil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>black pepper shallot cornflour cayenne pepper ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine                           ingredients_lemma_string\n",
       "0        greek  romaine lettuce black olive grape tomato garli...\n",
       "1  southern_us  plain flour ground pepper salt tomato ground b...\n",
       "2     filipino  egg pepper salt mayonaise cooking oil green ch...\n",
       "3       indian                     water vegetable oil wheat salt\n",
       "4       indian  black pepper shallot cornflour cayenne pepper ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame for training and validation\n",
    "traindf = rawdf_tr[['cuisine', 'ingredients_lemma_string']].reset_index(drop=True)\n",
    "print(traindf.shape)\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9944, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_lemma_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18009</th>\n",
       "      <td>baking powder egg all purpose flour raisin mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28583</th>\n",
       "      <td>sugar egg yolk corn starch cream of tartar ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41580</th>\n",
       "      <td>sausage link fennel bulb frond olive oil cuban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29752</th>\n",
       "      <td>meat cut file powder smoked sausage okra shrim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35687</th>\n",
       "      <td>ground black pepper salt sausage casing leek p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ingredients_lemma_string\n",
       "id                                                      \n",
       "18009  baking powder egg all purpose flour raisin mil...\n",
       "28583  sugar egg yolk corn starch cream of tartar ban...\n",
       "41580  sausage link fennel bulb frond olive oil cuban...\n",
       "29752  meat cut file powder smoked sausage okra shrim...\n",
       "35687  ground black pepper salt sausage casing leek p..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = rawdf_te[['ingredients_lemma_string']]\n",
    "print(testdf.shape)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training ===================\n",
    "# X_train\n",
    "X_train_ls = traindf['ingredients_lemma_string']\n",
    "vectorizertr = TfidfVectorizer(stop_words='english', analyzer=\"word\", max_df=0.65, min_df=2, binary=True)\n",
    "X_train = vectorizertr.fit_transform(X_train_ls)\n",
    "\n",
    "# y_train\n",
    "y_train = traindf['cuisine']\n",
    "# for xgboost the labels need to be labeled with encoder\n",
    "le = LabelEncoder()\n",
    "y_train_ec = le.fit_transform(y_train)\n",
    "\n",
    "# predicting =================\n",
    "# X_test\n",
    "X_pred_ls = testdf['ingredients_lemma_string']\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')\n",
    "X_pred = vectorizertr.transform(X_pred_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation - metric  \n",
    "  \n",
    "As I have previously mentioned, I originally planned to use AUC-ROC to measure the performance for the models but I decided to stick with [Kaggle's original guidline](https://www.kaggle.com/c/whats-cooking#evaluation) to measure each model based on the categorization accuracy (the percent of dishes that you correctly classify).  \n",
    "\n",
    "Additionally, I will be using sklearn's grid search with cross validation to search for the best parameters for the models (except for Keras deep learning model) that I will be using. The cost of running time and tweaking is very high (about a week).\n",
    "\n",
    "Please refer to the file: **best_estimators.py** or **best_estimators.ipynb** for the GridSearchCV for details.\n",
    "  \n",
    "#### Implementation - algorithms (models) I will be using:\n",
    "\n",
    "This [page](http://scikit-learn.org/stable/modules/multiclass.html) gives a clear guide on sklearn's available algorithms that I may be able to use. Among all the algorithms, I choose to implement the following from sklearn:\n",
    " - Random Forest (sklearn.ensemble.[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
    " - Naive Bayes (sklearn.naive_bayes.[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html))\n",
    " - Logistic Regression (sklearn.linear_model.[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression))\n",
    " - Neural Network by sklearn (sklearn.neural_network.[MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier))\n",
    " - Logistic Regression (sklearn.linear_model.[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)) with the one-vs-all wrapper by sklearn\n",
    " - Support Vector Classification (sklearn.svm.[SVC¶](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)) with the one-vs-all wrapper by sklearn\n",
    " - SGD (sklearn.linear_model.[SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)) with the one-vs-all wrapper by sklearn\n",
    "\n",
    "Additionally, I will be implementing:  \n",
    " - [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
    " - [LightGBM](https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/sklearn.html#LGBMClassifier)\n",
    " - [Keras](https://keras.io/layers/core/) Deep Neural Network (not in this notebook: please refer to the file: **chef_keras_deep_cooking.py** or **chef_keras_deep_cooking.ipynb** for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a function to save the csv files for submittions to Kaggle for results\n",
    "def submit_csv(y_pred_model, name):\n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['id'] = testdf.index.values\n",
    "    submit_df['cuisine'] = y_pred_model\n",
    "    submit_df.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "clf_rf = RandomForestClassifier()\n",
    "# fit the training data\n",
    "clf_rf = clf_rf.fit(X_train, y_train)\n",
    "# make prediction on the cuisine\n",
    "y_pred_rf = clf_rf.predict(X_pred)\n",
    "\n",
    "# save the predicted cuisine into a csv file per Kaggle guideline\n",
    "submit_csv(y_pred_rf, 'random_forest_benchmark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented the base model using sklearn's random forest without adjusting any parameters. I received a result of 0.70002 (around top 80% on the public leaderboard on Kaggle when the competition was active) - I think it is not bad for my first Kaggle score without doing any tuning on the parameters and being able to use the sklearn's base random forest model.\n",
    "\n",
    "After GridSearchCV with a validation score of 0.7595 accuracy, Kaggle gives a new score of 0.75905 (around top 63%) for the tuned random forest model. My score went up almost 20% on the ladder! To achieve a higher accuracy I must explore other models! So I have used the same technique (GridSearchCV) to tune all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the best parameters from grid search results for each models to fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest\n",
    "\n",
    "I tuned the max_features, criterion, and number of estimators among all the parameters. I set the number of estimators very high because there are a high number of ingredients. I let the model to test between 'gini' and 'entropy' as criterion, and also different number of features when looking for the best split.\n",
    "\n",
    "I have also kept the random state as 0,and n_jobs as 2 to keep 2 cores for computing. The validation accuracy score is 0.7595."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the random forest model\n",
    "clf_rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=2,\n",
    "            oob_score=True, random_state=0, verbose=0, warm_start=False)\n",
    "# fit the training data\n",
    "clf_rf = clf_rf.fit(X_train, y_train)\n",
    "# make prediction on the cuisine\n",
    "y_pred_rf = clf_rf.predict(X_pred)\n",
    "\n",
    "# save the predicted cuisine into a csv file per Kaggle guideline\n",
    "submit_csv(y_pred_rf, 'random_forest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes\n",
    "\n",
    "The sklearn's naive bayes class has a module for multinomial models. It is designed for classification with discrete features (e.g. word counts for text classification) - perfect for this problem. With text classification, it requires integer feature counts or tf-idf input.\n",
    "\n",
    "Alpha is the only parameter that I have tuned. I tried [0.01, 0.02, 0.035, 0.04, 0.1, 0.5, 1] and found that 0.035 gives the highest validation score - 0.7424."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the Multinomial Naive Bayes model\n",
    "clf_mnb = MultinomialNB(alpha=0.035, class_prior=None, fit_prior=True)\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_mnb, 'naive_bayes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "Sklearn has many variations of logistic regression models. I implemented and tuned the basic model. I tried many different solver and the 'C', which is the inverse of regularization strength. I found the combination of lbfgs and 5 gives me the best validation score - 0.7896."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the Logistic Regression (Multinomial) model\n",
    "clf_lr = LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
    "          n_jobs=2, penalty='l2', random_state=0, solver='lbfgs',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "clf_lr = clf_lr.fit(X_train,y_train)\n",
    "y_pred_lr = clf_lr.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_lr, 'logistic_regression_base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network by sklearn\n",
    "\n",
    "It was surprising and unsurprising to find the neural network model on sklearn. I expected to build a neural network myself using keras (which I did later) but I found this handy tool straight from sklearn. I was also unsurprised to find it on sklearn because they are very well known for providing the most comprehensive and organized machine learning API's.\n",
    "\n",
    "There are many parameters that I have fiddled with such as the activation function, solver, and the learning rate. It gave a decent validation score of 0.7811."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the sklearn's neural network (MLPclassifier) model\n",
    "clf_mlp = MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=0, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "       warm_start=False)\n",
    "clf_mlp = clf_mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = clf_mlp.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_mlp, 'NN_MLP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression with OVA\n",
    "\n",
    "This is the second logsitic regression I implemented with the multi-class classification strategy of One-Versus-All. One-Versus-All is a classic multi-class classification strategies based on this [material](http://www.mit.edu/~9.520/spring09/Classes/multiclass.pdf) and [Wikipeida](https://en.wikipedia.org/wiki/Multiclass_classification). Sklearn provides a wrapper classification method, sklearn.multiclass.[OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier), to turn binary classifiers (SVM, SGD, etc.) into multi-class classifiers.\n",
    "\n",
    "Like the previous logistic regression base model, I tuned the solver and the 'C' parameters and received a validation score of 0.7958."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the One-Versus-All Logistic Regression model\n",
    "clf_ovrc_lr = OneVsRestClassifier(estimator=LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
    "          penalty='l2', random_state=0, solver='lbfgs', tol=0.0001,\n",
    "          verbose=0, warm_start=False),\n",
    "          n_jobs=2)\n",
    "clf_ovrc_lr = clf_ovrc_lr.fit(X_train, y_train)\n",
    "y_pred_ovrc_lr = clf_ovrc_lr.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_ovrc_lr, 'logistic_regression_ova.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM with OVA\n",
    "\n",
    "Another model that I have implemented with the OVA wrapper. I tuned a number of parameters including the 'C', 'gamma', 'coef0', cache size, and kernel. This grid search took a very long time to run (about 11.5 hours on 6 cores of the CPU). Finally, I received an high validation score of 0.8082 (the highest so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the One-Versus-All SVM model\n",
    "clf_ovrc_svm = SVC(C=3.25, cache_size=500, class_weight=None, coef0=0.0,\\\n",
    "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\\\n",
    "  max_iter=-1, probability=False, random_state=0, shrinking=True,\\\n",
    "  tol=0.001, verbose=False)\n",
    "clf_ovrc_svm = clf_ovrc_svm.fit(X_train, y_train)\n",
    "y_pred_ovrc_svm = clf_ovrc_svm.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_ovrc_svm, 'SVM_ova.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD with OVA\n",
    "\n",
    "Stochastic Gradient Descent has been my favorite classification method for its efficiency and performance. I tried to tune 2 of the parameters: loss function and the learning rate. With the modified huber loss and optimal learning rate, I received a validation score of 0.7797 (not bad but not the best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the One-Versus-All SGD model\n",
    "clf_ovrc_sgd = OVRC(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='modified_huber', max_iter=None,\n",
    "       n_iter=None, n_jobs=2, penalty='l2', power_t=0.5, random_state=0,\n",
    "       shuffle=True, tol=None, verbose=0, warm_start=False), n_jobs=2)\n",
    "clf_ovrc_sgd = clf_ovrc_sgd.fit(X_train, y_train)\n",
    "y_pred_ovrc_sgd = clf_ovrc_sgd.predict(X_pred)\n",
    "\n",
    "submit_csv(y_pred_ovrc_sgd, 'SGD_ova.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "\n",
    "It is always exciting to explore something not in the textbook. XGBoost was recommended by a Udacity mentor. The API is straight-forward to read. It had a constraint on the input data - labels should be integers instead of strings. Therefore, I used sklearn's LabelEncoder to fit the labels and later inverse transformed the labels back to strings as cuisine prediction.\n",
    "\n",
    "I tuned a few parameters: learning rate, max depth of the tree, gamma, and number of estimators. This took a quite long time to grid search (13.5 hours on 2 cores of the CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator after running the grid search cross validation for the XGBoost model\n",
    "clf_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=1, learning_rate=0.01, max_delta_step=0,\n",
    "       max_depth=12, min_child_weight=1, missing=None, n_estimators=1000,\n",
    "       n_jobs=2, nthread=None, objective='multi:softprob', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.8)\n",
    "clf_xgb = clf_xgb.fit(X_train, y_train_ec)\n",
    "y_pred_xgb = clf_xgb.predict(X_pred)\n",
    "# inverse transform the encoded cuisine column\n",
    "y_pred_xgb = le.inverse_transform(y_pred_xgb)\n",
    "\n",
    "submit_csv(y_pred_xgb, 'XGBoost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM\n",
    "\n",
    "Lke XGBoost, lightgbm was also introduced by the Udacity mentor. It is a similar algorithm as XGBoost. I tuned the learning rate and the number of estimators. However, it never stopped running... I gave up after the 20th hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "clf_gbm = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "        gamma=1, learning_rate=0.01, max_depth=6, min_child_samples=20,\n",
    "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n",
    "        n_jobs=2, num_leaves=31, objective='multiclass', random_state=0,\n",
    "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=0.8,\n",
    "        subsample_for_bin=200000, subsample_freq=0)\n",
    "clf_gbm = clf_gbm.fit(X_train, y_train_ec)\n",
    "y_pred_gbm = clf_gbm.predict(X_pred)\n",
    "y_pred_gbm = le.inverse_transform(y_pred_gbm)\n",
    "\n",
    "submit_csv(y_pred_gbm, 'LightGBM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Deep Learning\n",
    "\n",
    "With [Keras documentation](https://keras.io/getting-started/sequential-model-guide/), [guide1](https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/), [guide2](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/), and [guide3](https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/), I have successfully designed my neural network for text classification.\n",
    "\n",
    "I started with the relu activation function with about half of the nodes with the input function. A 0.5 dropout layer is followed after it. Then I dense again with a tanh function followed by a higher disgard rate dropout layer (0.67) to reduce dimension. Lastly, I condense the data with softmax and 20 nodes (20 cuisine) as outputs. I set the epoch (max iteration) as 10 while set the early stopping as patience as 1 by monitoring the total validation loss in each iteration.\n",
    "\n",
    "Here is the detailed code for the architecture:\n",
    "```\n",
    "# define the layers\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(2182,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(144, activation='tanh'))\n",
    "model.add(Dropout(0.67))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# parameters for neural network\n",
    "epochs = 10\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "```\n",
    "\n",
    "Here is the results after fitting the training data:\n",
    "\n",
    "```\n",
    "Train on 33799 samples, validate on 5975 samples\n",
    "Epoch 1/10\n",
    " - 13s - loss: 1.0982 - acc: 0.6840 - val_loss: 0.7255 - val_acc: 0.7868\n",
    "Epoch 2/10\n",
    " - 13s - loss: 0.7309 - acc: 0.7840 - val_loss: 0.6944 - val_acc: 0.7883\n",
    "Epoch 3/10\n",
    " - 13s - loss: 0.6135 - acc: 0.8191 - val_loss: 0.6721 - val_acc: 0.8012\n",
    "Epoch 4/10\n",
    " - 13s - loss: 0.5297 - acc: 0.8427 - val_loss: 0.6842 - val_acc: 0.7975\n",
    "```\n",
    "The validation score is 0.7975, which is higher than all other models!\n",
    "\n",
    "I learned the following while trying different architectures for neural network:\n",
    " 1. It does not require deep and complex layers for best output\n",
    " 2. Dropout layer with a high disgard rate prevent the model from overfitting very efficiently.\n",
    " 3. ReLU is the best activation function for the first layer\n",
    " 4. Middle layers can be implemented with tanh, softsign, or sigmoid, and the differences are minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and justification (sorted by Kaggle's score):\n",
    "\n",
    "| **Model** | **Grid Search Validation Score** |**Kaggle's Accuracy Score after submission** |\n",
    "|----|----|----|\n",
    "| SVM (One-Vs-All) | 0.8082 | 0.81114 |\n",
    "| Neural Network by Keras | 0.7975 | 0.79444 |\n",
    "| Logistic Regression (Multinomial) | 0.7896 | 0.78660 |\n",
    "| Logistic Regression (One-Vs-All) | 0.7958 | 0.78620 |\n",
    "| Neural Network (MLP Classifier by sklearn) | 0.7811 | 0.78278 |\n",
    "| Linear Classifiers with SGD | 0.7797 | 0.78057 |\n",
    "| XGBoost | 0.7710 | 0.77896 |\n",
    "| Random Forest (tuned) | 0.7595 | 0.75905 |\n",
    "| LightGBM | never finished running | 0.74064 |\n",
    "| Naive Bayes (Multinomial) | 0.7424 | 0.73793 |\n",
    "| **Random Forest (benchmark)** | -- | **0.70002** |\n",
    "\n",
    "All the models end up producing much better results than the benchmark model (random forest out of the box). SVM (OVA) produced the highest accuracy **0.81114** which would have ended up on the **top 10%** of [Kaggle's Leaderboard](https://www.kaggle.com/c/whats-cooking/leaderboard) 2 years ago when the competition was still active. The deep learning with keras scored the second highest as **0.79444**, followed by the logistic regression base model.\n",
    "\n",
    "As expected, the model with the highest validation score scored the highest on the testing data (SVM). The final models are reasonable and align with the solution expectations. The final parameters are appropriate and robust enough for the problem and the datasets. The result is evaluated by Kaggle after the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "\n",
    "### Reflection\n",
    "When I encountered this problem during research, I found it very intriguing because I am a foodie and a home chef who is passionate about food. I thought it would be appropriate to apply what I have learned through the Udacity's Machine Learning nanodegree on this classic classification problem by using all the models and techniques that I have learned (supervised learning, sampling, validation, parameter tuning, deep learning, and more!).\n",
    "\n",
    "As I approach this problem, I found myself still lack knowledge and experiences in preprocessing. I am spoiled with preprocessed clean data and all I had to do is to 'fit' it with a out-of-box sklearn module. I can imagine that in the industry, many problems has to be solved with some serous configurations of discovered models, or even made from scretch.\n",
    "\n",
    "I also learned many different strategies of classifying dataset with multiple classes: one-versus-all, pairs, and error-correction-output-coding. Sklearn also have the error-correction-output-code wrapper to turn binary classification algorithms into multi-class ones. I tried it on Logistic Regression, SVM, and SGD while ran into problems of configuring the input/output. Since the result was not competitive (less than 70% accuracy) I disgarded them from this project.\n",
    "\n",
    "This is also the first time that I encountered natural language processing problem. Lemmatizing/stemming and the tf-idf vectorizering technique are very useful in dealing with text classification and reducing dimensions before feeding the classification models. I found it challenging to generalize the patterns in the ingredient list. To best disregard the 'noises' in the training and testing dataset, we might have to define our own grammar rules. In the earlier section, I mentioned that I was ready to remove all non-nouns in the ingredients list with the pos_tag function labeled words. However, the labeled words were wrongly labeled sometimes. For example, one of the ingredient was '2 1/2 to 3 lb. chicken, cut into serving pieces'. The word 'chicken' was labeled as a verb instead of a noun, while the main ingredient in this string is the chicken. Chicken is the only ingredient mattered in this long string. Selecting only the nouns among the string will run the risk of giving up many key ingredients for correctly predicting the cuisine.\n",
    "\n",
    "I did not enjoy the time waiting for the grid search results. In the beginning, I set the n_jobs parameters as high as possible, so '-1', which would be all 6 cores of my i5 - 8400 CPU. During the validation time, the computer freezes up often when I tried to open a new webpage for research. For future training, I will explore the option of reducing the number of core running or even seeking resource from AWS. I also planned to use KNN as one of my algorithms but the cross validation time was too long even for 10% of the data. I think it requires a lot of memory (which my computer does not have) for the full dataset.\n",
    "\n",
    "I have definitely learned a lot from this experience since it was my first Kaggle project. It was rewarding when I found my scores are competitive enough on the leaderboard. I also enjoyed reading some of the posting (kernels) on Kaggle website.\n",
    "\n",
    "### Improvement\n",
    "There are many areas for improvements or considerations for the future projects:\n",
    " 1. AUC_ROC is still a very good metric to implement for imbalanced dataset. I tried to implement it in the grid search with the make_scorer function and the scoring parameter, but it did not work properly as expected. I shall consider implementing it in the future along with accuracy to measure the performance for each model.\n",
    " 2. I shall consider dig deeper in configuring my own [corpus](http://www.nltk.org/howto/corpus.html#creating-new-corpus-reader-instances) using the NLTK library for future NLP problems. If I can capture all the essential ingredients for each cuisine, the accuracy score will be able to improve substantially.\n",
    " 3. I believe there must be a way of designing a better neural network architecture using keras, and the new neural network model can beat the SVM's accuracy score.\n",
    " 4. I was going to try combining some algorithms together into a custom ensemble model with [help1](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python), [help2](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/) but I had trouble implementing it. The key of stacking the algorithms is to use the predictions of two or more algorithms as features for another algorithm as input before predicting the final labels. I had trouble putting features of tf-idf vectorizing, data fold diving, and cross validation together into a function. I also had trouble identifying how well each model predicting on each cuisine. To achieve this, I should learn by doing projects with smaller scale on datasets and follow the [help1](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) and [help2](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/) tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "kitchen",
   "language": "python",
   "name": "kitchen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
