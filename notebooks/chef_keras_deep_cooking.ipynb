{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Kaggle- What's Cooking\n",
    "Qixiang Zhang  \n",
    "Jul 3rd, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "##### EXPLORE #########==================\n",
    "# data exploring and basic libraries\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import deque as dq\n",
    "\n",
    "# NLP preprocessing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize as TK\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##### MODELING ######===================\n",
    "# from time import time\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dfb2ebc3b61f38d770827538f80316d6f55c62b4"
   },
   "outputs": [],
   "source": [
    "# load the data - test data\n",
    "rawdf_te = pd.read_json(path_or_buf='raw_data/test.json').set_index('id')\n",
    "rawdf_tr = pd.read_json(path_or_buf='raw_data/train.json').set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess (regular expression + lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "57e02b453cf2bcc24102f09b27656828237a6d06"
   },
   "outputs": [],
   "source": [
    "# substitute the matched pattern\n",
    "def sub_match(pattern, sub_pattern, ingredients):\n",
    "    for i in ingredients.index.values:\n",
    "        for j in range(len(ingredients[i])):\n",
    "            ingredients[i][j] = re.sub(pattern, sub_pattern, ingredients[i][j].strip())\n",
    "            ingredients[i][j] = ingredients[i][j].strip()\n",
    "    re.purge()\n",
    "    return ingredients\n",
    "\n",
    "def regex_sub_match(series):\n",
    "    # remove all units\n",
    "    p0 = re.compile(r'\\s*(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\s*[^a-z]')\n",
    "    series = sub_match(p0, ' ', series)\n",
    "    # remove all digits\n",
    "    p1 = re.compile(r'\\d+')\n",
    "    series = sub_match(p1, ' ', series)\n",
    "    # remove all the non-letter characters\n",
    "    p2 = re.compile('[^\\w]')\n",
    "    series = sub_match(p2, ' ', series)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the series from the dataframe\n",
    "ingredients_tr = rawdf_tr['ingredients']\n",
    "# do the test.json while at it\n",
    "ingredients_te = rawdf_te['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4c77dbfd0b2ff4b7e5cd797fe4e2a94a7932b2c3"
   },
   "outputs": [],
   "source": [
    "# regex train data\n",
    "ingredients_tr = regex_sub_match(ingredients_tr)\n",
    "# regex test.json data\n",
    "ingredients_te = regex_sub_match(ingredients_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0a8b552ed1768d8b045d9517199761f21301613c"
   },
   "outputs": [],
   "source": [
    "# declare instance from WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove all the words that are not nouns -- keep the essential ingredients\n",
    "def lemma(series):\n",
    "    for i in series.index.values:\n",
    "        for j in range(len(series[i])):\n",
    "            # get rid of all extra spaces\n",
    "            series[i][j] = series[i][j].strip()\n",
    "            # Tokenize a string to split off punctuation other than periods\n",
    "            token = TK(series[i][j])\n",
    "            # set all the plural nouns into singular nouns\n",
    "            for k in range(len(token)):\n",
    "                token[k] = lemmatizer.lemmatize(token[k])\n",
    "            token = ' '.join(token)\n",
    "            # write them back\n",
    "            series[i][j] = token\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "dd4a84519e050991010b1f7e05cf6ae14232a47c"
   },
   "outputs": [],
   "source": [
    "# lemmatize the train data\n",
    "ingredients_tr = lemma(ingredients_tr)\n",
    "# lemmatize test.json\n",
    "ingredients_te = lemma(ingredients_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "bb3259388d2d7dc128dc06281c66592f56c06924"
   },
   "outputs": [],
   "source": [
    "# copy back to the dataframe\n",
    "rawdf_tr['ingredients_lemma'] = ingredients_tr\n",
    "rawdf_tr['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_tr['ingredients_lemma']]\n",
    "# do the same for the test.json dataset\n",
    "rawdf_te['ingredients_lemma'] = ingredients_te\n",
    "rawdf_te['ingredients_lemma_string'] = [' '.join(_).strip() for _ in rawdf_te['ingredients_lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9a5de65a674b2f0630b8f787fae92b6cc2685ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindf:  (33799, 2)\n",
      "validdf:  (5975, 2)\n",
      "(9944, 1)\n"
     ]
    }
   ],
   "source": [
    "# basically train_test_split customized to input cuisine name, outputs are 2 lists of indicies for train and test for the cuisine\n",
    "def tt_split(cuisine):\n",
    "    cuisine_population = rawdf_tr.loc[(rawdf_tr['cuisine'] == cuisine)].index.values\n",
    "    train, valid = train_test_split(cuisine_population, test_size=0.15, random_state=0)\n",
    "    train = train.tolist()\n",
    "    valid = valid.tolist()\n",
    "    return train, valid\n",
    "\n",
    "cuisine_list = rawdf_tr['cuisine'].unique().tolist()\n",
    "# split the training data into 85-15\n",
    "ix_train = [] # 85% for training (and validation)\n",
    "ix_valid = [] # 15% for hold-out test\n",
    "for _ in cuisine_list:\n",
    "    temp_train, temp_valid = tt_split(_)\n",
    "    ix_train += temp_train\n",
    "    ix_valid += temp_valid\n",
    "\n",
    "# DataFrame for training and validation\n",
    "traindf = rawdf_tr[['cuisine', 'ingredients_lemma_string']].loc[ix_train].reset_index(drop=True)\n",
    "print('traindf: ', traindf.shape)\n",
    "validdf = rawdf_tr[['cuisine', 'ingredients_lemma_string']].loc[ix_valid].reset_index(drop=True)\n",
    "print('validdf: ', validdf.shape)\n",
    "    \n",
    "# 85% for training and validation ===================\n",
    "# X_train\n",
    "X_train_ls = traindf['ingredients_lemma_string']\n",
    "vectorizertr = TfidfVectorizer(stop_words='english', analyzer=\"word\", max_df=0.65, min_df=2, binary=True)\n",
    "X_train = vectorizertr.fit_transform(X_train_ls)\n",
    "\n",
    "# y_train\n",
    "y_train = traindf['cuisine']\n",
    "le = LabelEncoder()\n",
    "y_train_ec = le.fit_transform(y_train)\n",
    "# 1-hot encoding for keras input deep learning\n",
    "y_train_1h = pd.get_dummies(y_train_ec)\n",
    "\n",
    "# save the 15% data for hold-out test ===============\n",
    "# X_pred\n",
    "X_valid_ls = validdf['ingredients_lemma_string']\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')\n",
    "X_valid = vectorizertr.transform(X_valid_ls)\n",
    "\n",
    "# y_true\n",
    "y_valid = validdf['cuisine']\n",
    "y_valid_ec = le.fit_transform(y_valid)\n",
    "# 1-hot encoding for keras input deep learning\n",
    "y_valid_1h = pd.get_dummies(y_valid_ec)\n",
    "\n",
    "# prediction test dataframe ==========================\n",
    "testdf = rawdf_te[['ingredients_lemma_string']]\n",
    "print(testdf.shape)\n",
    "testdf.head(n=2)\n",
    "# predicting =================\n",
    "# X_test\n",
    "X_test_ls = testdf['ingredients_lemma_string']\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')\n",
    "X_test = vectorizertr.transform(X_test_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9555622de72410dcc116707bb7759c8cfad1ae32",
    "collapsed": true
   },
   "source": [
    "## Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "f5190b0a4a35875c5609cbf08a024eb0408a5aab"
   },
   "outputs": [],
   "source": [
    "# define the layers\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(2182,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(144, activation='tanh'))\n",
    "model.add(Dropout(0.67))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for neural network\n",
    "epochs = 10\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33799 samples, validate on 5975 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 1.0982 - acc: 0.6840 - val_loss: 0.7255 - val_acc: 0.7868\n",
      "Epoch 2/10\n",
      " - 13s - loss: 0.7309 - acc: 0.7840 - val_loss: 0.6944 - val_acc: 0.7883\n",
      "Epoch 3/10\n",
      " - 13s - loss: 0.6135 - acc: 0.8191 - val_loss: 0.6721 - val_acc: 0.8012\n",
      "Epoch 4/10\n",
      " - 13s - loss: 0.5297 - acc: 0.8427 - val_loss: 0.6842 - val_acc: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24bbb1cd208>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train,y_train_1h,\n",
    "          batch_size=20,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(X_valid, y_valid_1h),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qixia\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# make prediction based on the model\n",
    "y_test_nn = le.inverse_transform(model.predict_classes(X_test))\n",
    "\n",
    "# save the output to a csv file\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df['id'] = testdf.index.values\n",
    "submit_df['cuisine'] = y_test_nn\n",
    "submit_df.to_csv('Neural_Network.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0a62320c6076869b427f403a3082cac7324b172"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
